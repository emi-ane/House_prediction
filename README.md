ğŸ  House Price Prediction and Classification Project

ğŸ“Œ Project Overview

This project aims to accurately predict house prices and classify them into pricing categories (low, medium, high) using state-of-the-art regression and classification algorithms.
It leverages a cleaned and feature-engineered housing dataset to develop high-performing machine learning models through rigorous tuning, validation, and visualization.


ğŸ¯ Objectives

 -ğŸ”¢ Predict house prices using regression models (Ridge, SVR, XGBoost Regressor)

 -ğŸ¿ Classify houses into pricing categories using classification models (Random Forest, LightGBM, XGBoost Classifier)

 -ğŸ” Handle data imbalance and feature skewness

 -âš™ï¸ Perform hyperparameter tuning to improve model performance

 -ğŸ“Š Visualize predictions and model decision boundaries


ğŸ› ï¸ Tools & Libraries Used

 -Python, NumPy, Pandas, Matplotlib, Seaborn

 -Scikit-learn, XGBoost, LightGBM, imblearn

 -t-SNE for dimensionality reduction


ğŸ§¹ Data Preprocessing

 -âœ… Removed irrelevant features

 -ğŸ§® Applied log transformation to price column for regression

 -ğŸ›¡ï¸ Created new features (e.g., total bathrooms, property age)

 -ğŸš« Handled missing values

 -ğŸ“Š Normalized numerical features for regressors

 -â™»ï¸ Balanced target classes using SMOTE for classification


 ğŸ§ª Models and Results

  - âŸ³ Regression Models (Log-transformed price target)

| Model           | Train RÂ²   | Test RÂ²    | Test RMSE   | Notes                                   |
| --------------- | ---------- | ---------- | ----------- | --------------------------------------- |
| Ridge (Tuned)   | 0.9978     | 0.8423     | 530,839     | Excellent generalization                |
| SVR (Tuned)     | 0.8296     | 0.8409     | 210,000     | Slightly less fit than Ridge            |
| XGBoost (Tuned) | **0.9186** | **0.8397** | **210,000** | Best tradeoff of fit and generalization |

  âœ… XGBoost Regressor emerged as the most balanced and performant model for price prediction.


  -  ğŸ§  Classification Models (Price categories: 0, 1, 2)
  
| Model                      | Accuracy   | Class 0 F1 | Class 1 F1 | Class 2 F1 | Notes                                 |
| -------------------------- | ---------- | ---------- | ---------- | ---------- | ------------------------------------- |
| Random Forest (Tuned)      | 0.8235     | 0.87       | 0.82       | 0.68       | Good balance across classes           |
| LightGBM (Tuned)           | 0.8200     | 0.86       | 0.82       | 0.67       | Slightly weaker on minority class     |
| XGBoost Classifier (Tuned) | **0.8321** | **0.87**   | **0.83**   | **0.74**   | **Best overall classification model** |

   âœ… XGBoost Classifier was the strongest performer in classification, particularly after SMOTE and hyperparameter tuning.


ğŸ“‰ Visualizations

  -ğŸ§« t-SNE and PCA showed good class separation potential

  -ğŸ“Œ Confusion matrices revealed class 2 (high-price) was hardest to predict

  -ğŸ” Sample predictions demonstrated models' close approximation to real values


âš ï¸ Challenges & Fixes ğŸ’¡

 -ğŸ§â€â™‚ï¸ Imbalanced Classes (Classification)

   -ğŸ‘‰ Problem: Class 2 (high-price) had fewer examples
   -âœ… Fix: Applied SMOTE to oversample the minority class

 -ğŸ§  Overfitting in Untuned Models

   -ğŸ‘‰ Problem: High training accuracy but poor generalization
   -âœ… Fix: Tuned models using GridSearchCV to improve validation performance

 -âŸ³ Inconsistent Results When Rerunning Cells

   -ğŸ‘‰ Problem: Variables retained from earlier runs interfered with new predictions
   -âœ… Fix: Used %reset_selective and reran training cells in order


âœ… Model Testing

Predicted samples from XGBoost Classifier and Regressor were compared with actuals and showed good alignment, demonstrating real-world predictive power.

-ğŸ“‹ Sample Results

  -Regression:
   Sample 1:
    Actual Price:    $535,000.00
    Predicted Price: $482,803.03
   Sample 2:
    Actual Price:    $445,000.00
    Predicted Price: $430,307.06

  -Classification:
  Sample 1:
   Actual Class:    1
   Predicted Class: 0
  Sample 2:
   Actual Class:    0
   Predicted Class: 0

ğŸ’¾ Conclusion

This project showcased a comprehensive approach to house price analysis through both regression and classification.

The best regression model was Tuned XGBoost Regressor, with a Test RÂ² of 0.84 and lowest RMSE.

For classification, Tuned XGBoost Classifier slightly outperformed others with an accuracy of 83.2%.

Between regression and classification, regression is more informative when exact pricing is needed, while classification is useful for grouping into price ranges.

Thanks to extensive preprocessing, feature engineering, and model tuning, the final models achieved robust and reliable performance that can be confidently deployed
or further integrated into a pricing tool or real estate platform.


ğŸ‘©â€ğŸ’» Author
emi-ane



